```markdown
# Faker API Data Flow

A Dockerized pipeline that ingests synthetic person data from a public API, anonymizes PII, stores it in DuckDB, and generates analytical reports using SQL and Streamlit.

---

## Project Overview

This project simulates a real-world data ingestion pipeline that processes external third-party data securely and efficiently. The pipeline fetches synthetic user data from the [Faker API](https://fakerapi.it/), anonymizes sensitive fields (PII), stores the transformed data in a local DuckDB database, and generates SQL-based reports for analytical insights.

It demonstrates:
- Scalable data ingestion from external APIs
- PII anonymization through masking and generalization
- SQL reporting on anonymized datasets
- Containerized orchestration with Docker and Apache Airflow

This solution was developed as part of a **Senior Data Engineer take-home challenge** and focuses on production-simulated design and code quality.

âž¡ï¸ For a discussion on **production-ready improvements**, see [Production Readiness](#-production-readiness--improvements).  
âž¡ï¸ To run this project locally, see [How to Run the Project](#-how-to-run-the-project).

---

## Features

- Ingests 30,000 synthetic user profiles from the Faker API
- Applies **PII masking** before storing raw data into partitioned Parquet files
  - Stored in `data_lake/raw/YYYY/MM/DD/`
  - Masks name, phone, address, coordinates, ZIP code, and email local part
- Applies **data generalization and cleaning** before loading into DuckDB
  - Converts birthdate to 10-year age groups (e.g., [30â€“40])
  - Extracts email domain and retains anonymized location details
- Loads cleaned data into DuckDB for querying
  - Database stored in a mounted `/db/` volume for persistence
- Generates SQL-based analytical reports:
  - % of German users using Gmail
  - Top 3 countries using Gmail
  - Count of Gmail users over age 60
- Visualizes results with a Streamlit dashboard
- Fully orchestrated with Apache Airflow
- Fully containerized with Docker for consistent local development

---

## Folder Structure

```

faker-api-dataflow/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ ETLUserMetrics/
â”‚   â”‚       â”œâ”€â”€ config/
â”‚   â”‚       â”‚   â”œâ”€â”€ anonymization_config.py
â”‚   â”‚       â”‚   â””â”€â”€ pipeline_config.py
â”‚   â”‚       â”œâ”€â”€ pr_utils/
â”‚   â”‚       â”‚   â”œâ”€â”€ anonymize.py
â”‚   â”‚       â”‚   â”œâ”€â”€ fetch.py
â”‚   â”‚       â”‚   â”œâ”€â”€ storage.py
â”‚   â”‚       â”‚   â”œâ”€â”€ transformation.py
â”‚   â”‚       â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚       â”‚   â””â”€â”€ operators/
â”‚   â”‚       â”‚       â””â”€â”€ duckdb_operator.py
â”‚   â”‚       â”œâ”€â”€ sql/
â”‚   â”‚       â”‚   â””â”€â”€ internal/
|   |       |       â””â”€â”€create_tables.sql
|   |       |       â””â”€â”€unique_email_provider_count.sql
|   |       |       â””â”€â”€init_internals_tables.sql                   
â”‚   â”‚       â”‚   â””â”€â”€ reporting/
|   |       |       â””â”€â”€germany_gmail_percentage.sql
|   |       |       â””â”€â”€over60_gmail_users.sql
|   |       |       â””â”€â”€top_gmail_countries.sql
â”‚   â”‚       â””â”€â”€ etl_user_metrics_dag.py
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ sql/
â”œâ”€â”€ data_lake/
â”‚   â””â”€â”€ raw/                               # Partitioned Parquet (YYYY/MM/DD)
â”œâ”€â”€ db/                                    # DuckDB database 
â”œâ”€â”€ gx/                                    # Placeholder for Great Expectations
â”œâ”€â”€ scripts/
â”œâ”€â”€ streamlit_app/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/ETLUserMetrics/
â”‚       â”œâ”€â”€ test_anonymize.py
â”‚       â”œâ”€â”€ test_fetch.py
â”‚       â””â”€â”€ test_sql_queries.py
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ airflow.db
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## â–¶ï¸ How to Run the Project

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/faker-api-dataflow.git
cd faker-api-dataflow
````

### 2. Set Up Environment Variables

```bash
cp .env.example .env
```

*(This is also handled automatically when you run `make up-all`.)*

### 3. Start the Project

```bash
make up-all
```

* Airflow UI: [http://localhost:8080](http://localhost:8080)
* Streamlit Dashboard: [http://localhost:8501](http://localhost:8501)

### 4. Trigger the DAG

1. Go to the Airflow UI.
2. Enable and trigger the DAG named: `user_metrics_pipeline`

### 5. View Results

* Raw masked data: `data_lake/raw/YYYY/MM/DD/persons.parquet`
* Anonymized + transformed data: `db/faker.duckdb`
* Final reports: Streamlit dashboard

### 6. Run Tests
Build the Docker test image and run all tests using `pytest`:

```bash
make all
```

---

## Data Flow Overview

```
start
  |
  â”œâ”€â”€> init_internal_tables
  |
  â””â”€â”€> fetch_and_anonymize
           |
           â”œâ”€â”€> cleanup_metadata_log
           |
           â””â”€â”€> transform
                    |
                    â”œâ”€â”€> top_gmail_countries
                    â”œâ”€â”€> over60_gmail_users
                    â””â”€â”€> germany_gmail_percentage
                             |
                            end
```

---

## Tech Stack

**Language & Frameworks**

* Python 3.10
* Apache Airflow â€“ DAG orchestration
* Streamlit â€“ Interactive reporting dashboard

**Data Handling**

* Pandas â€“ Data manipulation and transformation
* DuckDB â€“ Embedded analytical database
* Parquet â€“ Partitioned data lake storage

**Infrastructure & Tooling**

* Docker & Docker Compose â€“ Containerized setup
* Makefile â€“ Dev automation
* Environment variables â€“ Config management with `.env`

**Testing**

* Pytest â€“ Unit tests for pipeline modules
* Structured test suite under `/tests/`

---

## Example Reports

All reports run via SQL and are visualized in Streamlit.

**1. Germany Gmail Users (%):**

```sql
SELECT 
  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM persons_anonymized), 2) AS germany_gmail_percentage
FROM persons_anonymized
WHERE country = 'Germany' AND email = 'gmail.com';
```

**2. Top 3 Gmail Countries:**

```sql
SELECT country, COUNT(*) AS gmail_users
FROM persons_anonymized
WHERE email = 'gmail.com'
GROUP BY country
ORDER BY gmail_users DESC
LIMIT 3;
```

**3. Gmail Users Over 60:**

```sql
SELECT COUNT(*) AS gmail_over_60
FROM persons_anonymized
WHERE email = 'gmail.com' AND age_group IN ('[60-70]', '[70-80]', '[80-90]', '[90+]');
```

---

## ðŸš€ Production Readiness & Improvements

This project simulates a real-world data pipeline. Below are improvements and production-grade considerations:

### Data Quality & Schema Stability

* Schema drift detection via stored schema signatures (future work)
* Great Expectations integration planned for:

  * Null checks
  * Row count thresholds
  * Domain/value validation
* Formal data contracts for ingestion

### Pipeline Design

* Handle late-arriving data using watermarking or partition repair
* Retain raw data for reprocessing/backfilling
* Support re-runs and backfills gracefully
* Avoid table drops; use migration/versioning

### ðŸ”§ Architecture & Modularity

* Automate everything via Makefile, Docker, Airflow, and future CI/CD

### Observability & Monitoring

* Metadata logging for each run
* Future Slack/email alerting for failures or drift
* Track KPIs: DAG runtime, freshness, volume

### Cloud Readiness

* Cost-aware practices for cloud DBs
* Use partition pruning, materialized views
* Optimize latency for near-real-time pipelines

### Testing & Documentation

* Full unit tests and SQL query validation
* Future: GitHub Actions pipeline

```